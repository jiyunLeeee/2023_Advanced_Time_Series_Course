# -*- coding: utf-8 -*-
"""보고서] 실증분석.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17WTi9BZtLUbjr2rzQrJK7RgL3Lad_HoZ
"""

# 데이터 파일의 한글과 그래프에서 한글을 깨짐 없이 보기 위해 아래 코드를 실행한 후 런타임을 재시작한다.
!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import random
import os

import math
from math import sqrt

import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib as mpl
from tqdm import tqdm
import pickle

import time
import re

# Ts test & plot
import scipy.stats as stats
import statsmodels.graphics.tsaplots as sgt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.seasonal import seasonal_decompose
import statsmodels.api as sm
from scipy.stats import boxcox

# Model
! pip install pmdarima
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error
from pmdarima.arima import auto_arima
from statsmodels.tsa.statespace.sarimax import SARIMAX

# LSTM
! pip install scikit-optimize
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from hyperopt import fmin, tpe, hp


import warnings
warnings.filterwarnings("ignore")

# Commented out IPython magic to ensure Python compatibility.
# 폰트 깨짐 방지
# %matplotlib inline

import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

#!sudo apt-get install -y fonts-nanum
#!sudo fc-cache -fv
#!rm ~/.cache/matplotlib -rf

plt.rc('font', family = 'NanumBarunGothic')

# palette 설정
sns.set_palette("pastel")

def seed_everything(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

seed_everything(42) # Seed 고정

print(torch.cuda.is_available())
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

def eval(actual, predicted):
    rmse = sqrt(mean_squared_error(actual, predicted))
    mae = mean_absolute_error(actual, predicted)
    mape = mean_absolute_percentage_error(actual, predicted)

    return {'rmse' : rmse,
            'mae' : mae,
            'mape' : mape}

"""#### 1. 데이터"""

# Endog Data
elec = pd.read_csv('/content/drive/MyDrive/[고급시계열] 기말발표/데이터/실증분석/200201~202309전력사용량.csv')
elec.head()

elec = elec.drop(['cntr', 'custCnt', 'bill', 'metro', 'city'], axis = 1)
elec = pd.DataFrame(elec.groupby(['year', 'month'])['powerUsage','unitCost'].sum())

exo_1 = elec['unitCost']
start_date = '20020101' ; end_date = '20230930'
elec['시점'] = pd.date_range(start = start_date, end = end_date, freq = 'M')
elec = elec.reset_index(drop = True)

elec

exo_2 = pd.read_csv('/content/drive/MyDrive/[고급시계열] 기말발표/데이터/실증분석/200201~202310평균상대습도.csv', encoding = 'cp949')

print(exo_1.head())
print(exo_2.head())

"""#### 2. [Endog] 시각화 및 전처리
- 시각화
- VST
- Detrend
"""

plt.figure(figsize=(15, 5))
plt.plot(elec['시점'],elec['powerUsage'])
plt.title("서울시 가정용 전력 사용량 (2002년 1월 ~ 2023년 9월)")
plt.xlabel("년도")
plt.ylabel("사용량 (MWh)")

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 2.5))
plot_acf(elec['powerUsage'], lags = 40, ax = ax1)
ax1.set_xlabel('Lags')
ax1.set_ylabel('ACF')
plt.title('ACF')

plot_pacf(elec['powerUsage'], lags = 40, ax = ax2)
ax2.set_xlabel('Lags')
ax2.set_ylabel('PACF')
plt.title('PACF')

plt.show()

elec

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# 예제 데이터 생성 (시간과 전력 사용량을 포함한 데이터프레임)
# 예제 데이터가 없으므로 가상의 데이터를 사용하셔야 합니다.
# elec = pd.read_csv('your_data.csv')

# 시계열 분해
decomposition_6 = seasonal_decompose(elec['powerUsage'], period=12)

# 추세 및 계절성을 제외한 부분 추출
detrended_series = decomposition_6.observed - decomposition_6.trend

# 전체 추세 및 계절성 제거 시각화
plt.figure(figsize=(15.4, 5))
plt.plot(elec['시점'], detrended_series)
plt.title('Detrended: d=6')
plt.show()

# ACF 및 PACF 시각화
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 2.5))

plot_acf(detrended_series.dropna(), lags=40, ax=ax1)
ax1.set_xlabel('Lags')
ax1.set_ylabel('ACF')
ax1.set_title('ACF of Detrended Series')

plot_pacf(detrended_series.dropna(), lags=40, ax=ax2)
ax2.set_xlabel('Lags')
ax2.set_ylabel('PACF')
ax2.set_title('PACF of Detrended Series')

plt.show()

# 계절성 제거 (계절 주기가 12인 경우)
seasonal_difference = detrended_series.diff(12).dropna()

# 시각화
plt.figure(figsize=(15, 5))
plt.plot(seasonal_difference)
plt.title('Detrended and Seasonally Differenced: d=12, D=1')
plt.show()

# ACF 및 PACF 시각화
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 2.5))

plot_acf(seasonal_difference, lags=40, ax=ax1)
ax1.set_xlabel('Lags')
ax1.set_ylabel('ACF')
ax1.set_title('ACF of Seasonally Differenced Series')

plot_pacf(seasonal_difference, lags=40, ax=ax2)
ax2.set_xlabel('Lags')
ax2.set_ylabel('PACF')
ax2.set_title('PACF of Seasonally Differenced Series')

plt.show()

# ADF test
from statsmodels.tsa.stattools import adfuller

def adf_test(timeseries):
    print("Results of Dickey-Fuller Test:")
    dftest = adfuller(timeseries, autolag="AIC")
    dfoutput = pd.Series(
        dftest[0:4],
        index=[
            "Test Statistic",
            "p-value",
            "#Lags Used",
            "Number of Observations Used",
        ],
    )
    for key, value in dftest[4].items():
        dfoutput["Critical Value (%s)" % key] = value
    print(dfoutput)

adf_test(elec['powerUsage']) # 정상시계열이 아니다 (맞는 말임)

# Normality Check (QQ-plot)
plt.figure(figsize = (10, 5))

res = stats.probplot(elec['powerUsage'], plot = plt)

plt.title('Normal QQ Plot for powerUsage')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Ordered Values')
plt.show()

# 1) Log Transformation
data_log = np.log(elec['powerUsage'])

plt.figure(figsize=(15, 3))
plt.plot(elec['시점'], data_log)
plt.title('Log Transformed Time Series')
plt.xlabel('년도')

plt.figure(figsize = (10, 5))
res_log = stats.probplot(data_log, plot = plt)
plt.title('Normal QQ Plot for Log Transformed powerUsage')
plt.xlabel('Theoretical Quantiles')
plt.ylabel('Ordered Values')

plt.show()

# 시계열 분해
decomposition_12 = seasonal_decompose(data_log, period=12)

# 추세 및 계절성을 제외한 부분 추출
detrended_series = decomposition_12.observed - decomposition_12.trend

# 전체 추세 및 계절성 제거 시각화
plt.figure(figsize=(15.4, 5))
plt.plot(elec['시점'], detrended_series)
plt.title('Detrended: d=6')
plt.show()

# ACF 및 PACF 시각화
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 2.5))

plot_acf(detrended_series.dropna(), lags=40, ax=ax1)
ax1.set_xlabel('Lags')
ax1.set_ylabel('ACF')
ax1.set_title('ACF of Detrended Series')

plot_pacf(detrended_series.dropna(), lags=40, ax=ax2)
ax2.set_xlabel('Lags')
ax2.set_ylabel('PACF')
ax2.set_title('PACF of Detrended Series')

plt.show()

# 계절성 제거 (계절 주기가 12인 경우)
seasonal_difference = detrended_series.diff(12).dropna()

# 시각화
plt.figure(figsize=(15, 5))
plt.plot(seasonal_difference)
plt.title('Detrended and Seasonally Differenced: d=12, D=1')
plt.show()

# ACF 및 PACF 시각화
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 2.5))

plot_acf(seasonal_difference, lags=40, ax=ax1)
ax1.set_xlabel('Lags')
ax1.set_ylabel('ACF')
ax1.set_title('ACF of Seasonally Differenced Series')

plot_pacf(seasonal_difference, lags=40, ax=ax2)
ax2.set_xlabel('Lags')
ax2.set_ylabel('PACF')
ax2.set_title('PACF of Seasonally Differenced Series')

plt.show()

"""#### 3. 외생변수 1

##### SARIMA & SARIMAX
"""

train_data = np.array(data_third[:255])
test_data = np.array(data_third[255:])

model_sarima = auto_arima(data_third[:255],
                     m = 12,
                     trace=True,
                     error_action = "ignore",
                     suppress_warnings = True,
                     stepwise = True,
                     information_criterion = 'aic')
print(model_sarima.summary())

order = (0, 1, 0) ; seasonal_order = (0, 0, 0, 12)

# SARIMAX 모델 학습
sarima_model = SARIMAX(endog = np.array(data_third[:255]),
                        order=order,
                        seasonal_order = seasonal_order,
                        enforce_stationarity = False)
sarima_result = sarima_model.fit()

# 테스트 데이터 예측
forecast_sarima = sarima_result.get_forecast(steps = 6)

model_sarimax_1 = auto_arima(data_third[:255],
                     exog = exo_1[:255],
                     m = 12,
                     trace = True,
                     error_action = "ignore",
                     suppress_warnings = True,
                     stepwise = True,
                     information_criterion = 'aic')
print(model_sarimax_1.summary())

order = (0, 1, 0) ; seasonal_order = (0, 0, 0, 12)

# SARIMAX 모델 학습
model_sarimax_1 = SARIMAX(endog = np.array(data_third[:255]),
                        exog = exo_1[:255],
                        order = order,
                        seasonal_order = seasonal_order,
                        enforce_stationarity = False)
sarimax_result_1 = model_sarimax_1.fit()

# 테스트 데이터 예측
forecast_sarimax_1 = sarimax_result_1.get_forecast(steps = 6, exog = exo_1[255:])

# 실제값과 예측값 비교
eval(data_log[255:], forecast_sarimax_1.predicted_mean)

print(sarimax_result_1.summary())

plt.figure(figsize=(15, 2))
plt.plot(test_data)
plt.plot(np.array(forecast_sarimax_1.predicted_mean))
plt.legend(('Actual','Predicted'))
plt.title('Actual vs. Predicted')
plt.show()

sarima_predicted = np.array(forecast_sarima.predicted_mean)
sarima_predicted

sarimax_predicted = np.array(forecast_sarimax_1.predicted_mean)
sarimax_predicted

"""##### LSTM"""

elec['log_trans'] = data_log

# Min-Max Scaler로 0~1사이로 정규화
mMscaler_train = MinMaxScaler()
mMscaler_train.fit(elec[['log_trans']][:255])
mMscaled_data_train = mMscaler_train.transform(elec[['log_trans']][:255])

mMscaler_test = MinMaxScaler()
mMscaler_test.fit(elec[['log_trans']][255:])
mMscaled_data_test = mMscaler_test.transform(elec[['log_trans']][255:])

def split_data_into_X_and_y_1(data, look_back):
    X, y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i:i+look_back])
        y.append(data[i+look_back])
    return np.array(X), np.array(y)

class LSTMmodel(nn.Module):
    def __init__(self, input_size, num_classes, num_fc_layers, num_lstm_node, num_fc_node, dropout_rate, num_layers):
        super(LSTMmodel, self).__init__()

        self.num_fc_layers = int(num_fc_layers)
        self.num_lstm_node = int(num_lstm_node)
        self.num_fc_node = int(num_fc_node)
        self.dropout_rate = dropout_rate
        self.input_size = int(input_size)
        self.num_classes = int(num_classes)
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.num_lstm_node, num_layers=self.num_layers, batch_first=True)
        self.dropout = nn.Dropout(p=dropout_rate)

        self.fc_layers = nn.ModuleList([])

        if self.num_fc_layers >= 1:
            self.fc_layers.append(nn.Linear(self.num_lstm_node, self.num_fc_node))
            if self.num_fc_layers > 1:
                for _ in range(self.num_fc_layers - 1):
                    self.fc_layers.append(nn.Linear(self.num_fc_node, self.num_fc_node))

            self.output_layer = nn.Linear(self.num_fc_node, self.num_classes)
        else:
            self.output_layer = nn.Linear(self.num_lstm_node, self.num_classes)

        self.relu = nn.ReLU()

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.num_lstm_node).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.num_lstm_node).to(x.device)
        out, (h0, c0) = self.lstm(x, (h0, c0))
        h0 = h0.view(-1, self.num_lstm_node)
        out = out[:, -1, :]

        # Experiment with different activation functions
        out = self.relu(out)

        for layer in self.fc_layers:
            out = layer(out)
            out = self.dropout(out)
            out = self.relu(out)

        out = self.output_layer(out)

        return out

def train(learning_rate, num_fc_layers, num_lstm_node, num_fc_node, dropout_rate, num_classes):
    model = LSTMmodel(input_size = X_train.shape[2],
                 output_size = 1,
                 num_fc_layers = num_fc_layers,
                 num_lstm_node = num_lstm_node,
                 num_fc_node = num_fc_node,
                 dropout_rate = dropout_rate,
                 num_layers = 2,
                num_classes = 1)
    model.to(device)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr = learning_rate)

    num_epochs = 1000
    early_stopping_patience = 50
    best_valid_loss = np.inf
    no_improvement = 0

    for epoch in range(num_epochs):
        model.train()  # 모델을 훈련 모드로 설정
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)  # 데이터를 GPU로 이동

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        if epoch % 10 == 0:
          print("epoch : %d, loss = %1.5f" %(epoch,loss.item()))

        model.eval()  # 모델을 평가 모드로 설정
        valid_loss = 0

        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)  # 데이터를 GPU로 이동
                outputs = model(inputs)
                valid_loss += criterion(outputs, labels).item()
        valid_loss /= len(test_loader.dataset)

        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            no_improvement = 0
            torch.save(model.state_dict(), 'best_model.pth')  # 최고 성능 모델 저장
        else:
            no_improvement += 1

        if no_improvement >= early_stopping_patience:
            print(f'Early stopping after {epoch + 1} epochs with no improvement.')
            break

    return best_valid_loss

space = {
    'learning_rate': hp.uniform('learning_rate', 0.000001, 0.0001),
    'num_fc_layers': hp.quniform('num_fc_layers', 0, 2, 1),
    'num_lstm_node': hp.quniform('num_lstm_node', 16, 128, 16),
    'num_fc_node': hp.quniform('num_fc_node', 16, 128, 16),
    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5)
}

def objective(params):
    try:
        test_loss = train(params['learning_rate'],
                                        params['num_fc_layers'],
                                        params['num_lstm_node'],
                                        params['num_fc_node'],
                                        params['dropout_rate'])
        return test_loss

    except Exception as e:
        print("Error occurred:", e)
        return float('inf')

class LSTMmodel(nn.Module):
    def __init__(self, input_size, output_size, num_fc_layers, num_lstm_node, num_fc_node, dropout_rate, num_layers):
        super(LSTMmodel, self).__init__()

        self.num_fc_layers = int(num_fc_layers)
        self.num_lstm_node = int(num_lstm_node)
        self.num_fc_node = int(num_fc_node)
        self.dropout_rate = dropout_rate
        self.input_size = int(input_size)
        self.output_size = int(output_size)
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.num_lstm_node, num_layers=self.num_layers, batch_first=True)

        self.layer_1 = nn.Linear(self.num_lstm_node, self.num_fc_node)
        self.layer_2 = nn.Linear(self.num_fc_node, self.num_fc_node)
        self.layer_3 = nn.Linear(self.num_fc_node, self.num_fc_node)
        self.layer_out = nn.Linear(self.num_fc_node, self.output_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=dropout_rate)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.num_lstm_node).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.num_lstm_node).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        #out = out[:, -1, :]

        out = self.layer_1(out) # first layer
        out = self.relu(out) # activation func relu
        out = self.layer_2(out)
        out = self.relu(out)
        out = self.layer_3(out)
        out = self.relu(out)
        out = self.layer_out(out) #Output layer
        return out

def train(learning_rate, num_fc_layers, num_lstm_node, num_fc_node, dropout_rate):
    model = LSTMmodel(input_size=X_train.shape[2],
                 output_size=1,
                 num_fc_layers=num_fc_layers,
                 num_lstm_node=num_lstm_node,
                 num_fc_node=num_fc_node,
                 dropout_rate=dropout_rate,
                 num_layers=2)
    model.to(device)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    num_epochs = 1000
    early_stopping_patience = 50
    best_valid_loss = np.inf
    no_improvement = 0

    for epoch in range(num_epochs):
        model.train()
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        if epoch % 100 == 0:
            print("epoch : %d, loss = %1.5f" % (epoch, loss.item()))

        model.eval()
        valid_loss = 0

        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                valid_loss += criterion(outputs, labels).item()
        valid_loss /= len(test_loader.dataset)

        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            no_improvement = 0
            torch.save(model.state_dict(), 'best_model.pth')
        else:
            no_improvement += 1

        if no_improvement >= early_stopping_patience:
            print(f'Early stopping after {epoch + 1} epochs with no improvement.')
            break

    return best_valid_loss

space = {
    'learning_rate': hp.uniform('learning_rate', 0.00001, 0.001),
    'num_fc_layers': hp.quniform('num_fc_layers', 0, 2, 1),
    'num_lstm_node': hp.quniform('num_lstm_node', 32, 150, 32),
    'num_fc_node': hp.quniform('num_fc_node', 32, 150, 32),
    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5)
}

def objective(params):
    try:
        test_loss = train(params['learning_rate'],
                          params['num_fc_layers'],
                          params['num_lstm_node'],
                          params['num_fc_node'],
                          params['dropout_rate'])
        return test_loss

    except Exception as e:
        print("Error occurred:", e)
        return float('inf')

X, y = split_data_into_X_and_y_1(np.append(mMscaled_data_train.reshape(-1,1),mMscaled_data_test.reshape(-1,1)).reshape(-1,1), look_back = 6)
X_train, y_train = X[:243], y[:243]
X_val, y_val = X[243:249], y[243:249]
X_test, y_test = X[249:], y[249:]

X_train = torch.Tensor(X_train)
X_val = torch.Tensor(X_val)
X_test = torch.Tensor(X_test)

y_train = torch.Tensor(y_train)
y_val = torch.Tensor(y_val)
y_test = torch.Tensor(y_test)

train_data = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_data, batch_size = 14, shuffle=True)

val_data = TensorDataset(X_val, y_val)
test_loader = DataLoader(val_data, batch_size = 14)

best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 2)
print("Best Hyperparameters:", best)

best_model = LSTMmodel(input_size=X_train.shape[2],
                      output_size=1,
                      num_fc_layers=int(best['num_fc_layers']),
                      num_lstm_node=int(best['num_lstm_node']),
                      num_fc_node=int(best['num_fc_node']),
                      dropout_rate=best['dropout_rate'],
                      num_layers=2)

best_model.eval()
with torch.no_grad():
    y_pred = best_model(X_test)
    y_pred = y_pred.numpy()

eval(np.array(elec[['log_trans']][255:]), mMscaler_test.inverse_transform(y_pred[:6]))

lstm_predicted_no_exog_1 = np.array(mMscaler_test.inverse_transform(y_pred))
lstm_predicted_no_exog_1

np.exp(lstm_predicted_no_exog_1)

elec[['powerUsage']][255:]

plt.figure(figsize = (15, 7))

plt.plot(np.array(elec['powerUsage'][255:]))
plt.plot(np.array(np.exp(lstm_predicted_no_exog_1)))

"""###### with exog"""

elec['exog_1'] = np.array(exo_1)

X = elec[['log_trans', 'exog_1']]
y = elec['log_trans']

mMscaler_train = MinMaxScaler()
mMscaler_train.fit(X.iloc[:255])
mMscaled_data_train = mMscaler_train.transform(X.iloc[:255])

mMscaler_test = MinMaxScaler()
mMscaler_test.fit(y.iloc[255:].values.reshape(-1, 1))
mMscaled_data_test = mMscaler_test.transform(y.iloc[255:].values.reshape(-1, 1))

def train_test_split_1(data, time_step):
    X, y = [], []
    for i in range(len(data) - time_step):
        X.append(data[i:i+time_step])
        y.append(data[i+time_step])
    return X, y

X, y = train_test_split_1(np.append(mMscaled_data_train, mMscaled_data_test.reshape(-1, 1)).reshape(-1, 1), time_step=6)
X_train, y_train = X[:243], y[:243]
X_val, y_val = X[243:249], y[243:249]
X_test, y_test = X[249:], y[249:]

X_train = torch.Tensor(X_train)
X_val = torch.Tensor(X_val)
X_test = torch.Tensor(X_test)

y_train = torch.Tensor(y_train)
y_val = torch.Tensor(y_val)
y_test = torch.Tensor(y_test)

train_data = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_data, batch_size = 14, shuffle=True)

val_data = TensorDataset(X_val, y_val)
test_loader = DataLoader(val_data, batch_size = 14)

best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 3)
print("Best Hyperparameters:", best)

best_model = LSTMmodel(input_size=X_train.shape[2],
                      output_size = 1,
                      num_fc_layers=best['num_fc_layers'],
                      num_lstm_node=best['num_lstm_node'],
                      num_fc_node=best['num_fc_node'],
                      dropout_rate=best['dropout_rate'],
                      num_layers=2)
with torch.no_grad():
    y_pred = best_model(X_test)
    y_pred = y_pred.numpy()

eval(np.array(elec[['log_trans']][255:]), mMscaler_test.inverse_transform(y_pred)[:6])

lstm_predicted_exog_1 = np.array(mMscaler_test.inverse_transform(y_pred)[:6])
lstm_predicted_exog_1



"""##### Hybrid"""

orders = model_sarimax_1.order
seasonal_orders = model_sarimax_1.seasonal_order

def sign_param(data, orders, seasonal_orders):
    sarimax_m = SARIMAX(endog = data,
                        exog = np.array(exo_1),
                        order = orders,
                        seasonal_order = seasonal_orders,
                        initialization='approximate_diffuse').fit(disp=False)

    p_val = sarimax_m.pvalues
    param_list = p_val.index.tolist() #전체 파라미터
    sign_param_name = p_val[p_val <0.05].index.tolist()
    ar_param = sarimax_m.arparams
    ma_param = sarimax_m.maparams
    s_ar_param = sarimax_m.seasonalarparams
    s_ma_param = sarimax_m.seasonalmaparams
    gamma = sarimax_m.params[-1]

    param = pd.DataFrame(sarimax_m.params).loc[sign_param_name].values #유의한 파라미터
    param_coeff = sarimax_m.params.loc[sign_param_name].values
    res = sarimax_m.resid


    if len(param_list)==len(sign_param_name):
            sign_num = True
    else:
            sign_num = False

    result={'p-val':p_val,
            'all_significant':sign_num,
            'order':orders,
            'seasonal_order':seasonal_orders,
            'ar':ar_param.tolist(),
            'ma':ma_param.tolist(),
            'seasonal_ar':s_ar_param.tolist(),
            'seasonal_ma':s_ma_param.tolist(),
            'gamma':gamma,
            'significant_param_coeff': param_coeff.tolist(),
            'residual':res.tolist()}
    return result

sign_params = sign_param(data_log, orders, seasonal_orders)

sign_params['p-val'] # exogenous not significant

sign_params['significant_param_coeff']

exog = sign_params['significant_param_coeff'][0]
theta = sign_params['ma']
Theta = sign_params['seasonal_ma']
gamma = sign_params['gamma']
residuals = sign_params['residual']

# Min-Max Scaler로 0~1사이로 정규화
mMscaler_train = MinMaxScaler()
mMscaler_train.fit(elec[['log_trans']][:255])
mMscaled_data_train = mMscaler_train.transform(elec[['log_trans']][:255])

mMscaler_test = MinMaxScaler()
mMscaler_test.fit(elec[['log_trans']][255:])
mMscaled_data_test = mMscaler_test.transform(elec[['log_trans']][255:])

Z_t = np.append(mMscaled_data_train.reshape(-1,1), mMscaled_data_test.reshape(-1,1))
e_t = np.array(residuals)

Z_t_minus_1 = np.roll(Z_t, 1)
Z_t_minus_12 = np.roll(Z_t, 12)
Z_t_minus_13 = np.roll(Z_t, 13)

e_t_minus_1 = np.roll(e_t, 1)
e_t_minus_2 = np.roll(e_t, 2)
e_t_minus_12 = np.roll(e_t, 12)

# Compute the input array
input = np.vstack((Z_t_minus_1, Z_t_minus_12, -(Z_t_minus_13), gamma * np.array(exo_1), theta[0] * e_t_minus_1,
                  theta[1] * e_t_minus_2, Theta * e_t_minus_12))

input = input.reshape(261, 7)

def split_data_into_X_and_y_1(data, look_back):
    X, y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i:i+look_back])
        y.append(data[i+look_back])
    return np.array(X), np.array(y)

class LSTMmodel(nn.Module):
    def __init__(self, input_size, output_size, num_fc_layers, num_lstm_node, num_fc_node, dropout_rate, num_layers):
        super(LSTMmodel, self).__init__()

        self.num_fc_layers = int(num_fc_layers)
        self.num_lstm_node = int(num_lstm_node)
        self.num_fc_node = int(num_fc_node)
        self.dropout_rate = dropout_rate
        self.input_size = int(input_size)
        self.output_size = int(output_size)
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_size = self.input_size, hidden_size = self.num_lstm_node, num_layers = self.num_layers, batch_first = True)
        self.dropout = nn.Dropout(p = dropout_rate)

        self.fc_layers = nn.ModuleList([])

        if self.num_fc_layers >= 1:
            self.fc_layers.append(nn.Linear(self.num_lstm_node, self.num_fc_node))
            if self.num_fc_layers > 1:
                for _ in range(self.num_fc_layers - 1):
                    self.fc_layers.append(nn.Linear(self.num_fc_node, self.num_fc_node))

            self.output_layer = nn.Linear(self.num_fc_node, self.output_size)
        else:
            self.output_layer = nn.Linear(self.num_lstm_node, self.output_size)
        self.tanh = nn.Tanh()

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.num_lstm_node).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.num_lstm_node).to(x.device)
        out, (h_n, c_n) = self.lstm(x, (h0, c0))
        out = out[:, -1, :]

        out = self.tanh(out)
        out = self.dropout(out)

        for layer in self.fc_layers:
            out = layer(out)
            out = self.tanh(out)

        out = self.output_layer(out)

        return out

def train(learning_rate, num_fc_layers, num_lstm_node, num_fc_node, dropout_rate):
    model = LSTMmodel(input_size = X_train.shape[2],
                 output_size = 1,
                 num_fc_layers = num_fc_layers,
                 num_lstm_node = num_lstm_node,
                 num_fc_node = num_fc_node,
                 dropout_rate = dropout_rate,
                 num_layers = 2)
    model.to(device)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr= 0.009476459600149248)

    num_epochs = 1000
    early_stopping_patience = 50
    best_valid_loss = np.inf
    no_improvement = 0

    for epoch in range(num_epochs):
        model.train()  # 모델을 훈련 모드로 설정
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)  # 데이터를 GPU로 이동

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        model.eval()  # 모델을 평가 모드로 설정
        valid_loss = 0

        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)  # 데이터를 GPU로 이동
                outputs = model(inputs)
                valid_loss += criterion(outputs, labels).item()
        valid_loss /= len(test_loader.dataset)

        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            no_improvement = 0
            torch.save(model.state_dict(), 'best_model.pth')  # 최고 성능 모델 저장
        else:
            no_improvement += 1

        if no_improvement >= early_stopping_patience:
            print(f'Early stopping after {epoch + 1} epochs with no improvement.')
            break

    return best_valid_loss

space = {
    'learning_rate': hp.uniform('learning_rate', 0.001, 0.01),
    'num_fc_layers': hp.quniform('num_fc_layers', 0, 2, 1),
    'num_lstm_node': hp.quniform('num_lstm_node', 16, 128, 16),
    'num_fc_node': hp.quniform('num_fc_node', 16, 128, 16),
    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5)
}

def objective(params):
    try:
        test_loss = train(params['learning_rate'],
                                        params['num_fc_layers'],
                                        params['num_lstm_node'],
                                        params['num_fc_node'],
                                        params['dropout_rate'])
        return test_loss

    except Exception as e:
        print("Error occurred:", e)
        return float('inf')

def split_data_into_X_and_y_1(data, look_back):
    X, y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i:i+look_back])
        y.append(data[i+look_back])
    return np.array(X), np.array(y)

X, y = split_data_into_X_and_y_1(input, 6)
X_train, y_train = X[:243], y[:243]
X_val, y_val = X[243:249], y[243:249]
X_test, y_test = X[249:], y[249:]

X_train = torch.Tensor(X_train)
X_val = torch.Tensor(X_val)
X_test = torch.Tensor(X_test)

y_train = torch.Tensor(y_train)
y_val = torch.Tensor(y_val)
y_test = torch.Tensor(y_test)

train_data = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_data, batch_size = 14, shuffle=True)

val_data = TensorDataset(X_val, y_val)
test_loader = DataLoader(val_data, batch_size = 14)

best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 50)
print("Best Hyperparameters:", best)

best_model = LSTMmodel(input_size=X_train.shape[2],
                      output_size = 1,
                      num_fc_layers=0,
                      num_lstm_node=64,
                      num_fc_node=16,
                      dropout_rate = 0.3802504269016809,
                      num_layers=2)

with torch.no_grad():
    y_pred = best_model(X_test)
    y_pred = y_pred.numpy()

eval(np.array(data_log[255:]),mMscaler_test.inverse_transform(y_pred))

best_model = LSTMmodel(input_size=X_train.shape[2],
                      output_size = 1,
                      num_fc_layers=0,
                      num_lstm_node=64,
                      num_fc_node=16,
                      dropout_rate = 0.3802504269016809,
                      num_layers=2)

with torch.no_grad():
    y_pred = best_model(X_test)
    y_pred = y_pred.numpy()

eval(np.array(data_log[255:]),mMscaler_test.inverse_transform(y_pred))

eval(np.array(data_log[255:]),mMscaler_test.inverse_transform(y_pred))

hyrbid_coeff_predicted_1 = np.array(mMscaler_test.inverse_transform(y_pred))
hyrbid_coeff_predicted_1

"""##### plot"""

plt.figure(figsize = (15, 7))

plt.plot(np.array(elec['powerUsage'][255:]))
plt.plot(np.array(np.exp(sarima_predicted)))
plt.plot(np.array(np.exp(sarimax_predicted)))

plt.plot(np.array(np.exp(lstm_predicted_no_exog_1)))
plt.plot(np.array(np.exp(lstm_predicted_exog_1)))
plt.plot(np.array(np.exp(hyrbid_coeff_predicted_1)))

plt.legend(('Actual','SARIMA','SARIMAX','LSTM - no exog','LSTM - exog','Hybrid - coeff'))
plt.title('Actual vs  Predicted')
plt.show()

"""#### 외생변수 2

##### SARIMA & SARIMAX
"""

train_data = np.array(data_log[:255])
test_data = np.array(data_log[255:])

model_sarima = auto_arima(data_log[:255],
                     m = 12,
                     trace=True,
                     error_action = "ignore",
                     suppress_warnings = True,
                     stepwise = True,
                     information_criterion = 'aic')
print(model_sarima.summary())

order = (0, 1, 2) ; seasonal_order = (0, 1, 1, 12)

# SARIMAX 모델 학습
sarima_model = SARIMAX(endog = np.array(data_log[:255]),
                        order=order,
                        seasonal_order = seasonal_order,
                        enforce_stationarity = False)
sarima_result = sarima_model.fit()

# 테스트 데이터 예측
forecast_sarima = sarima_result.get_forecast(steps = 6)

model_sarimax_2 = auto_arima(data_log[:255],
                     exog = exo_2[:255],
                     m = 12,
                     trace = True,
                     error_action = "ignore",
                     suppress_warnings = True,
                     stepwise = True,
                     information_criterion = 'aic')
print(model_sarimax_2.summary())

order = (0, 1, 2) ; seasonal_order = (0, 1, 1, 12)

# SARIMAX 모델 학습
model_sarimax_2 = SARIMAX(endog = np.array(data_log[:255]),
                        exog = exo_2['평균상대습도(%)'][1:256],
                        order = order,
                        seasonal_order = seasonal_order,
                        enforce_stationarity = False)
sarimax_result_2 = model_sarimax_2.fit()

# 테스트 데이터 예측
forecast_sarimax_2 = sarimax_result_2.get_forecast(steps = 6, exog = exo_2['평균상대습도(%)'][256:])

# 실제값과 예측값 비교
eval(data_log[255:], forecast_sarimax_2.predicted_mean)

print(sarimax_result_2.summary())

plt.figure(figsize=(15, 2))
plt.plot(test_data)
plt.plot(np.array(forecast_sarimax_1.predicted_mean))
plt.legend(('Actual','Predicted'))
plt.title('Actual vs. Predicted')
plt.show()

sarima_predicted2 = np.array(forecast_sarima.predicted_mean)
sarima_predicted2

sarimax_predicted2 = np.array(forecast_sarimax_2.predicted_mean)
sarimax_predicted2

"""##### LSTM"""

# Min-Max Scaler로 0~1사이로 정규화
mMscaler_train = MinMaxScaler()
mMscaler_train.fit(elec[['log_trans']][:255])
mMscaled_data_train = mMscaler_train.transform(elec[['log_trans']][:255])

mMscaler_test = MinMaxScaler()
mMscaler_test.fit(elec[['log_trans']][255:])
mMscaled_data_test = mMscaler_test.transform(elec[['log_trans']][255:])

def split_data_into_X_and_y_1(data, look_back):
    X, y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i:i+look_back])
        y.append(data[i+look_back])
    return np.array(X), np.array(y)

class LSTMmodel(nn.Module):
    def __init__(self, input_size, output_size, num_fc_layers, num_lstm_node, num_fc_node, dropout_rate, num_layers):
        super(LSTMmodel, self).__init__()

        self.num_fc_layers = int(num_fc_layers)
        self.num_lstm_node = int(num_lstm_node)
        self.num_fc_node = int(num_fc_node)
        self.dropout_rate = dropout_rate
        self.input_size = int(input_size)
        self.output_size = int(output_size)
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_size = self.input_size, hidden_size = self.num_lstm_node, num_layers = self.num_layers, batch_first = True)
        self.dropout = nn.Dropout(p = dropout_rate)

        self.fc_layers = nn.ModuleList([])

        if self.num_fc_layers >= 1:
            self.fc_layers.append(nn.Linear(self.num_lstm_node, self.num_fc_node))
            if self.num_fc_layers > 1:
                for _ in range(self.num_fc_layers - 1):
                    self.fc_layers.append(nn.Linear(self.num_fc_node, self.num_fc_node))

            self.output_layer = nn.Linear(self.num_fc_node, self.output_size)
        else:
            self.output_layer = nn.Linear(self.num_lstm_node, self.output_size)
        self.tanh = nn.Tanh()

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.num_lstm_node).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.num_lstm_node).to(x.device)
        out, (h_n, c_n) = self.lstm(x, (h0, c0))
        out = out[:, -1, :]

        out = self.tanh(out)
        out = self.dropout(out)

        for layer in self.fc_layers:
            out = layer(out)
            out = self.tanh(out)

        out = self.output_layer(out)

        return out

def train(learning_rate, num_fc_layers, num_lstm_node, num_fc_node, dropout_rate):
    model = LSTMmodel(input_size = X_train.shape[2],
                 output_size = 1,
                 num_fc_layers = num_fc_layers,
                 num_lstm_node = num_lstm_node,
                 num_fc_node = num_fc_node,
                 dropout_rate = dropout_rate,
                 num_layers = 2)
    model.to(device)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr = learning_rate)

    num_epochs = 1000
    early_stopping_patience = 50
    best_valid_loss = np.inf
    no_improvement = 0

    for epoch in range(num_epochs):
        model.train()  # 모델을 훈련 모드로 설정
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)  # 데이터를 GPU로 이동

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        model.eval()  # 모델을 평가 모드로 설정
        valid_loss = 0

        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)  # 데이터를 GPU로 이동
                outputs = model(inputs)
                valid_loss += criterion(outputs, labels).item()
        valid_loss /= len(test_loader.dataset)

        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            no_improvement = 0
            torch.save(model.state_dict(), 'best_model.pth')  # 최고 성능 모델 저장
        else:
            no_improvement += 1

        if no_improvement >= early_stopping_patience:
            print(f'Early stopping after {epoch + 1} epochs with no improvement.')
            break

    return best_valid_loss

space = {
    'learning_rate': hp.uniform('learning_rate', 0.001, 0.01),
    'num_fc_layers': hp.quniform('num_fc_layers', 0, 2, 1),
    'num_lstm_node': hp.quniform('num_lstm_node', 16, 128, 16),
    'num_fc_node': hp.quniform('num_fc_node', 16, 128, 16),
    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5)
}

def objective(params):
    try:
        test_loss = train(params['learning_rate'],
                                        params['num_fc_layers'],
                                        params['num_lstm_node'],
                                        params['num_fc_node'],
                                        params['dropout_rate'])
        return test_loss

    except Exception as e:
        print("Error occurred:", e)
        return float('inf')

X, y = split_data_into_X_and_y_1(np.append(mMscaled_data_train.reshape(-1,1),mMscaled_data_test.reshape(-1,1)).reshape(-1,1), look_back = 6)
X_train, y_train = X[:243], y[:243]
X_val, y_val = X[243:249], y[243:249]
X_test, y_test = X[249:], y[249:]

X_train = torch.Tensor(X_train)
X_val = torch.Tensor(X_val)
X_test = torch.Tensor(X_test)

y_train = torch.Tensor(y_train)
y_val = torch.Tensor(y_val)
y_test = torch.Tensor(y_test)

train_data = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_data, batch_size = 14, shuffle=True)

val_data = TensorDataset(X_val, y_val)
test_loader = DataLoader(val_data, batch_size = 14)

best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 50)
print("Best Hyperparameters:", best)

best_model = LSTMmodel(input_size=X_train.shape[2],
                      output_size = 1,
                      num_fc_layers=best['num_fc_layers'],
                      num_lstm_node=best['num_lstm_node'],
                      num_fc_node=best['num_fc_node'],
                      dropout_rate=best['dropout_rate'],
                      num_layers=2)
with torch.no_grad():
    y_pred = best_model(X_test)
    y_pred = y_pred.numpy()

eval(np.array(elec[['log_trans']][255:]), mMscaler_test.inverse_transform(y_pred))

lstm_predicted_no_exog_2 = np.array(mMscaler_test.inverse_transform(y_pred))
lstm_predicted_no_exog_2

"""###### with exog"""

elec['exog_2'] = np.array(exo_2['평균상대습도(%)'][1:])

X = elec[['log_trans', 'exog_2']]
y = elec['log_trans']

mMscaler_train = MinMaxScaler()
mMscaler_train.fit(X.iloc[:255])
mMscaled_data_train = mMscaler_train.transform(X.iloc[:255])

mMscaler_test = MinMaxScaler()
mMscaler_test.fit(y.iloc[255:].values.reshape(-1, 1))
mMscaled_data_test = mMscaler_test.transform(y.iloc[255:].values.reshape(-1, 1))

def train_test_split_1(data, time_step):
    X, y = [], []
    for i in range(len(data) - time_step):
        X.append(data[i:i+time_step])
        y.append(data[i+time_step])
    return X, y

def train(learning_rate, num_fc_layers, num_lstm_node, num_fc_node, dropout_rate):
    model = LSTMmodel(input_size = X_train.shape[2],
                 output_size = 1,
                 num_fc_layers = num_fc_layers,
                 num_lstm_node = num_lstm_node,
                 num_fc_node = num_fc_node,
                 dropout_rate = dropout_rate,
                 num_layers = 2)
    model.to(device)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr = learning_rate)

    num_epochs = 1000
    early_stopping_patience = 50
    best_valid_loss = np.inf
    no_improvement = 0

    for epoch in range(num_epochs):
        model.train()  # 모델을 훈련 모드로 설정
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)  # 데이터를 GPU로 이동

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        model.eval()  # 모델을 평가 모드로 설정
        valid_loss = 0

        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)  # 데이터를 GPU로 이동
                outputs = model(inputs)
                valid_loss += criterion(outputs, labels).item()
        valid_loss /= len(test_loader.dataset)

        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            no_improvement = 0
            torch.save(model.state_dict(), 'best_model.pth')  # 최고 성능 모델 저장
        else:
            no_improvement += 1

        if no_improvement >= early_stopping_patience:
            print(f'Early stopping after {epoch + 1} epochs with no improvement.')
            break

    return best_valid_loss

X, y = train_test_split_1(np.append(mMscaled_data_train, mMscaled_data_test.reshape(-1, 1)).reshape(-1, 1), time_step=6)
X_train, y_train = X[:243], y[:243]
X_val, y_val = X[243:249], y[243:249]
X_test, y_test = X[249:], y[249:]

X_train = torch.Tensor(X_train)
X_val = torch.Tensor(X_val)
X_test = torch.Tensor(X_test)

y_train = torch.Tensor(y_train)
y_val = torch.Tensor(y_val)
y_test = torch.Tensor(y_test)

train_data = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_data, batch_size = 14, shuffle=True)

val_data = TensorDataset(X_val, y_val)
test_loader = DataLoader(val_data, batch_size = 14)

best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 50)
print("Best Hyperparameters:", best)

best_model = LSTMmodel(input_size=X_train.shape[2],
                      output_size = 1,
                      num_fc_layers=best['num_fc_layers'],
                      num_lstm_node=best['num_lstm_node'],
                      num_fc_node=best['num_fc_node'],
                      dropout_rate=best['dropout_rate'],
                      num_layers=2)
with torch.no_grad():
    y_pred = best_model(X_test)
    y_pred = y_pred.numpy()

eval(np.array(elec[['log_trans']][255:]), mMscaler_test.inverse_transform(y_pred)[:6])

best_model = LSTMmodel(input_size=X_train.shape[2],
                      output_size = 1,
                      num_fc_layers=2,
                      num_lstm_node=96,
                      num_fc_node=96,
                      dropout_rate=0.45407053217510074,
                      num_layers=2)
with torch.no_grad():
    y_pred = best_model(X_test)
    y_pred = y_pred.numpy()

eval(np.array(elec[['log_trans']][255:]), mMscaler_test.inverse_transform(y_pred)[:6])

lstm_predicted_exog_2 = np.array(mMscaler_test.inverse_transform(y_pred)[:6])
lstm_predicted_exog_2

"""##### Hybrid"""

orders = model_sarimax_2.order
seasonal_orders = model_sarimax_2.seasonal_order

def sign_param(data, orders, seasonal_orders):
    sarimax_m = SARIMAX(endog = data,
                        exog = np.array(exo_2['평균상대습도(%)'][1:]),
                        order = orders,
                        seasonal_order = seasonal_orders,
                        initialization='approximate_diffuse').fit(disp=False)

    p_val = sarimax_m.pvalues
    param_list = p_val.index.tolist() #전체 파라미터
    sign_param_name = p_val[p_val <0.05].index.tolist()
    ar_param = sarimax_m.arparams
    ma_param = sarimax_m.maparams
    s_ar_param = sarimax_m.seasonalarparams
    s_ma_param = sarimax_m.seasonalmaparams
    gamma = sarimax_m.params[-1]

    param = pd.DataFrame(sarimax_m.params).loc[sign_param_name].values #유의한 파라미터
    param_coeff = sarimax_m.params.loc[sign_param_name].values
    res = sarimax_m.resid


    if len(param_list)==len(sign_param_name):
            sign_num = True
    else:
            sign_num = False

    result={'p-val':p_val,
            'all_significant':sign_num,
            'order':orders,
            'seasonal_order':seasonal_orders,
            'ar':ar_param.tolist(),
            'ma':ma_param.tolist(),
            'seasonal_ar':s_ar_param.tolist(),
            'seasonal_ma':s_ma_param.tolist(),
            'gamma':gamma,
            'significant_param_coeff': param_coeff.tolist(),
            'residual':res.tolist()}
    return result

sign_params = sign_param(data_log, orders, seasonal_orders)

sign_params['p-val'] # exogenous not significant

sign_params['significant_param_coeff']

theta = sign_params['ma']
Theta = sign_params['seasonal_ma']
gamma = sign_params['gamma']
residuals = sign_params['residual']

# Min-Max Scaler로 0~1사이로 정규화
mMscaler_train = MinMaxScaler()
mMscaler_train.fit(elec[['log_trans']][:255])
mMscaled_data_train = mMscaler_train.transform(elec[['log_trans']][:255])

mMscaler_test = MinMaxScaler()
mMscaler_test.fit(elec[['log_trans']][255:])
mMscaled_data_test = mMscaler_test.transform(elec[['log_trans']][255:])

Z_t = np.append(mMscaled_data_train.reshape(-1,1), mMscaled_data_test.reshape(-1,1))
e_t = np.array(residuals)

Z_t_minus_1 = np.roll(Z_t, 1)
Z_t_minus_12 = np.roll(Z_t, 12)
Z_t_minus_13 = np.roll(Z_t, 13)

e_t_minus_1 = np.roll(e_t, 1)
e_t_minus_2 = np.roll(e_t, 2)
e_t_minus_12 = np.roll(e_t, 12)

# Compute the input array
input = np.vstack((Z_t_minus_1, Z_t_minus_12, -(Z_t_minus_13),  theta[0] * e_t_minus_1,
                  theta[1] * e_t_minus_2, Theta * e_t_minus_12))

input = input.reshape(261, 6)

def split_data_into_X_and_y_1(data, look_back):
    X, y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i:i+look_back])
        y.append(data[i+look_back])
    return np.array(X), np.array(y)

class LSTMmodel(nn.Module):
    def __init__(self, input_size, output_size, num_fc_layers, num_lstm_node, num_fc_node, dropout_rate, num_layers):
        super(LSTMmodel, self).__init__()

        self.num_fc_layers = int(num_fc_layers)
        self.num_lstm_node = int(num_lstm_node)
        self.num_fc_node = int(num_fc_node)
        self.dropout_rate = dropout_rate
        self.input_size = int(input_size)
        self.output_size = int(output_size)
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_size = self.input_size, hidden_size = self.num_lstm_node, num_layers = self.num_layers, batch_first = True)
        self.dropout = nn.Dropout(p = dropout_rate)

        self.fc_layers = nn.ModuleList([])

        if self.num_fc_layers >= 1:
            self.fc_layers.append(nn.Linear(self.num_lstm_node, self.num_fc_node))
            if self.num_fc_layers > 1:
                for _ in range(self.num_fc_layers - 1):
                    self.fc_layers.append(nn.Linear(self.num_fc_node, self.num_fc_node))

            self.output_layer = nn.Linear(self.num_fc_node, self.output_size)
        else:
            self.output_layer = nn.Linear(self.num_lstm_node, self.output_size)
        self.tanh = nn.Tanh()

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.num_lstm_node).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.num_lstm_node).to(x.device)
        out, (h_n, c_n) = self.lstm(x, (h0, c0))
        out = out[:, -1, :]

        out = self.tanh(out)
        out = self.dropout(out)

        for layer in self.fc_layers:
            out = layer(out)
            out = self.tanh(out)

        out = self.output_layer(out)

        return out

def train(learning_rate, num_fc_layers, num_lstm_node, num_fc_node, dropout_rate):
    model = LSTMmodel(input_size = X_train.shape[2],
                 output_size = 1,
                 num_fc_layers = num_fc_layers,
                 num_lstm_node = num_lstm_node,
                 num_fc_node = num_fc_node,
                 dropout_rate = dropout_rate,
                 num_layers = 2)
    model.to(device)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr= 0.007343870539835082)

    num_epochs = 1000
    early_stopping_patience = 50
    best_valid_loss = np.inf
    no_improvement = 0

    for epoch in range(num_epochs):
        model.train()  # 모델을 훈련 모드로 설정
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)  # 데이터를 GPU로 이동

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        model.eval()  # 모델을 평가 모드로 설정
        valid_loss = 0

        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)  # 데이터를 GPU로 이동
                outputs = model(inputs)
                valid_loss += criterion(outputs, labels).item()
        valid_loss /= len(test_loader.dataset)

        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            no_improvement = 0
            torch.save(model.state_dict(), 'best_model.pth')  # 최고 성능 모델 저장
        else:
            no_improvement += 1

        if no_improvement >= early_stopping_patience:
            print(f'Early stopping after {epoch + 1} epochs with no improvement.')
            break

    return best_valid_loss

space = {
    'learning_rate': hp.uniform('learning_rate', 0.001, 0.01),
    'num_fc_layers': hp.quniform('num_fc_layers', 0, 2, 1),
    'num_lstm_node': hp.quniform('num_lstm_node', 16, 128, 16),
    'num_fc_node': hp.quniform('num_fc_node', 16, 128, 16),
    'dropout_rate': hp.uniform('dropout_rate', 0.0, 0.5)
}

def objective(params):
    try:
        test_loss = train(params['learning_rate'],
                                        params['num_fc_layers'],
                                        params['num_lstm_node'],
                                        params['num_fc_node'],
                                        params['dropout_rate'])
        return test_loss

    except Exception as e:
        print("Error occurred:", e)
        return float('inf')

def split_data_into_X_and_y_1(data, look_back):
    X, y = [], []
    for i in range(len(data) - look_back):
        X.append(data[i:i+look_back])
        y.append(data[i+look_back])
    return np.array(X), np.array(y)

X, y = split_data_into_X_and_y_1(input, 6)
X_train, y_train = X[:243], y[:243]
X_val, y_val = X[243:249], y[243:249]
X_test, y_test = X[249:], y[249:]

X_train = torch.Tensor(X_train)
X_val = torch.Tensor(X_val)
X_test = torch.Tensor(X_test)

y_train = torch.Tensor(y_train)
y_val = torch.Tensor(y_val)
y_test = torch.Tensor(y_test)

train_data = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_data, batch_size = 14, shuffle=True)

val_data = TensorDataset(X_val, y_val)
test_loader = DataLoader(val_data, batch_size = 14)

best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 50)
print("Best Hyperparameters:", best)

best_model = LSTMmodel(input_size=X_train.shape[2],
                      output_size = 1,
                      num_fc_layers=best['num_fc_layers'],
                      num_lstm_node=best['num_lstm_node'],
                      num_fc_node=best['num_fc_node'],
                      dropout_rate=best['dropout_rate'],
                      num_layers=2)

with torch.no_grad():
    y_pred = best_model(X_test)
    y_pred = y_pred.numpy()

eval(np.array(data_log[255:]),mMscaler_test.inverse_transform(y_pred))

best_model = LSTMmodel(input_size=X_train.shape[2],
                      output_size = 1,
                      num_fc_layers=2,
                      num_lstm_node=64,
                      num_fc_node=128,
                      dropout_rate=0.47125173111725144,
                      num_layers=2)

with torch.no_grad():
    y_pred = best_model(X_test)
    y_pred = y_pred.numpy()

eval(np.array(data_log[255:]),mMscaler_test.inverse_transform(y_pred))

hyrbid_coeff_predicted_2 = np.array(mMscaler_test.inverse_transform(y_pred))
hyrbid_coeff_predicted_2

"""##### plot"""

plt.figure(figsize = (15, 7))

plt.plot(np.array(elec['powerUsage'][255:]))
plt.plot(np.array(np.exp(sarima_predicted2)))
plt.plot(np.array(np.exp(sarimax_predicted2)))

plt.plot(np.array(np.exp(lstm_predicted_no_exog_2)))
plt.plot(np.array(np.exp(lstm_predicted_exog_2)))
plt.plot(np.array(np.exp(hyrbid_coeff_predicted_2)))

plt.legend(('Actual','SARIMA','SARIMAX','LSTM - no exog','LSTM - exog','Hybrid - coeff'))
plt.title('Actual vs  Predicted')
plt.show()